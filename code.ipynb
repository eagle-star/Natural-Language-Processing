{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyltp import SentenceSplitter\n",
    "import functools\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from jieba import analyse\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#原始字节码转为字符串\n",
    "def byte2str(data):\n",
    "    pos = 0\n",
    "    str = ''\n",
    "    while pos < len(data):\n",
    "        c = chr(struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0])\n",
    "        if c != chr(0):\n",
    "            str += c\n",
    "        pos += 2\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取拼音表\n",
    "def getPyTable(data, GPy_Table, GTable):\n",
    "    data = data[4:]\n",
    "    pos = 0\n",
    "    while pos < len(data):\n",
    "        index = struct.unpack('H', bytes([data[pos],data[pos + 1]]))[0]\n",
    "        pos += 2\n",
    "        lenPy = struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0]\n",
    "        pos += 2\n",
    "        py = byte2str(data[pos:pos + lenPy])\n",
    "\n",
    "        GPy_Table[index] = py\n",
    "        pos += lenPy\n",
    "    return GPy_Table, GTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取一个词组的拼音\n",
    "def getWordPy(data, GPy_Table, GTable):\n",
    "    pos = 0\n",
    "    ret = ''\n",
    "    while pos < len(data):\n",
    "        index = struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0]\n",
    "        ret += GPy_Table[index]\n",
    "        pos += 2\n",
    "    return ret, GPy_Table, GTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取中文表\n",
    "def getChinese(data, GPy_Table, GTable):\n",
    "    pos = 0\n",
    "    while pos < len(data):\n",
    "        # 同音词数量\n",
    "        same = struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0]\n",
    "\n",
    "        # 拼音索引表长度\n",
    "        pos += 2\n",
    "        py_table_len = struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0]\n",
    "\n",
    "        # 拼音索引表\n",
    "        pos += 2\n",
    "        py, GPy_Table, GTable = getWordPy(data[pos: pos + py_table_len], GPy_Table, GTable)\n",
    "\n",
    "        # 中文词组\n",
    "        pos += py_table_len\n",
    "        for i in range(same):\n",
    "            # 中文词组长度\n",
    "            c_len = struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0]\n",
    "            # 中文词组\n",
    "            pos += 2\n",
    "            word = byte2str(data[pos: pos + c_len])\n",
    "            # 扩展数据长度\n",
    "            pos += c_len\n",
    "            ext_len = struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0]\n",
    "            # 词频\n",
    "            pos += 2\n",
    "            count = struct.unpack('H', bytes([data[pos], data[pos + 1]]))[0]\n",
    "\n",
    "            # 保存\n",
    "            GTable.append((count, py, word))\n",
    "\n",
    "            # 到下个词的偏移位置\n",
    "            pos += ext_len\n",
    "    return GPy_Table, GTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scel2txt(file_name, startPy, startChinese, GPy_Table, GTable):\n",
    "    print('-' * 60)\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # print(\"词库名：\", byte2str(data[0x130:0x338])) # .encode('GB18030')\n",
    "    # print(\"词库类型：\", byte2str(data[0x338:0x540]))\n",
    "    # print(\"描述信息：\", byte2str(data[0x540:0xd40]))\n",
    "    # print(\"词库示例：\", byte2str(data[0xd40:startPy]))\n",
    "    GPy_Table, GTable = getPyTable(data[startPy:startChinese], GPy_Table, GTable)\n",
    "    GPy_Table, GTable = getChinese(data[startChinese:], GPy_Table, GTable)\n",
    "    return GPy_Table, GTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词表加载方法\n",
    "def get_stopword_list():\n",
    "    #停用词表存储路径，每一行为一个词，按行读取进行加载\n",
    "    #进行编码转换确保匹配准确率\n",
    "    stop_word_path = '../Data/stopword.txt'\n",
    "    stop_word_list = [sw.replace('\\n', '') for sw in open(stop_word_path, encoding='utf-8').readlines()]\n",
    "    return stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词方法，调用结巴接口\n",
    "def jieba_seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        #不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除干扰词\n",
    "def jieba_word_filter(seg_list, stopword_list, pos=False):\n",
    "\n",
    "    filter_list = []\n",
    "    #根据pos参数选择是否词性过滤\n",
    "    #不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        #过滤高停用词表中的词，以及长度为<2的词\n",
    "        if not word in stopword_list and len(word) > 1:\n",
    "            filter_list.append(word)\n",
    "\n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_word_deal(sentence, stopword_list, pos=False):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = jieba_seg_to_list(sentence, pos)\n",
    "    filter_list = jieba_word_filter(seg_list, stopword_list, pos)\n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_title_word_n(sentence, stopword_list):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = jieba_seg_to_list(sentence, True)\n",
    "    title_word_n_list = []\n",
    "    for seg in seg_list:\n",
    "        word = seg.word\n",
    "        flag = seg.flag\n",
    "        if flag.startswith('n'):\n",
    "            title_word_n_list.append(word)\n",
    "    return title_word_n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词方法，调用ltp接口\n",
    "def ltp_seg_to_list(sentence, segmentor):\n",
    "    words = segmentor.segment(sentence)  # 分词\n",
    "    seg_list = list(words)\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除干扰词\n",
    "def ltp_word_filter(seg_list, stopword_list):\n",
    "\n",
    "    filter_list = []\n",
    "    #根据pos参数选择是否词性过滤\n",
    "    #不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        #过滤高停用词表中的词，以及长度为<2的词\n",
    "        if not seg in stopword_list and len(seg) > 1:\n",
    "            filter_list.append(seg)\n",
    "\n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltp_word_deal(sentence, stopword_list, segmentor):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = ltp_seg_to_list(sentence, segmentor)\n",
    "    filter_list = ltp_word_filter(seg_list, stopword_list)\n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_person_name(title_list, postagger):\n",
    "    attributes_list = list(postagger.postag(title_list))\n",
    "    title_person_name_list = list()\n",
    "    i = 0\n",
    "    for attributes in attributes_list:\n",
    "        if attributes == 'nh':\n",
    "            title_person_name_list.append(title_list[i])\n",
    "        i = i + 1\n",
    "    return title_person_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_sentences(text):\n",
    "    sentences_list = SentenceSplitter.split(text)\n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_text(all_docs_df):\n",
    "    temp_df = pd.DataFrame(columns=['id', 'title_text'])\n",
    "    for temp_id, title, text, text_sentences_len in all_docs_df[['id', 'title', 'text', 'text_sentences_len']].values:\n",
    "        length = math.ceil(text_sentences_len * 0.4)\n",
    "        if length < 6:\n",
    "            length = 6\n",
    "#         if length > 15:\n",
    "#             length = 15\n",
    "        title_text = ''\n",
    "        for i in range(length):\n",
    "            title_text = title + '。' + title_text\n",
    "        title_text = title_text + text\n",
    "        temp = pd.DataFrame([[temp_id, title_text]], columns=['id', 'title_text'])\n",
    "        temp_df = pd.concat([temp_df, temp])\n",
    "    all_docs_df = pd.merge(all_docs_df, temp_df, on='id', how='left')\n",
    "    return all_docs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf统计方法\n",
    "def train_idf(doc_list):\n",
    "    idf_dic = {}\n",
    "    #总文档数\n",
    "    tt_count = len(doc_list)\n",
    "\n",
    "    #每个词出现的文档数\n",
    "    for doc in doc_list:\n",
    "        for word in set(doc):\n",
    "            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "    #按公式转换为idf值，分母加1进行平滑处理\n",
    "    for k, v in idf_dic.items():\n",
    "        idf_dic[k] = math.log(tt_count / (1.0 + v))\n",
    "\n",
    "    #对于没有在字典中的词，默认其尽在一个文档中出现，得到默认idf值\n",
    "    default_idf = math.log(tt_count / (1.0))\n",
    "    return idf_dic, default_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#排序函数，用于topK关键词的按值排\n",
    "def cmp(e1, e2):\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a == b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF类\n",
    "class TfIdf(object):\n",
    "    #四个参数分别是：训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "        self.word_list = word_list\n",
    "        self.idf_dic = idf_dic\n",
    "        self.default_idf = default_idf\n",
    "        self.tf_dic = self.get_tf_dic()\n",
    "        self.keyword_num = keyword_num\n",
    "\n",
    "    #统计tf值\n",
    "    def get_tf_dic(self):\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count\n",
    "\n",
    "        return tf_dic\n",
    "\n",
    "    #按公式计算tf-idf\n",
    "    def get_tfidf(self):\n",
    "        tfidf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            idf = self.idf_dic.get(word, self.default_idf)\n",
    "            tf = self.tf_dic.get(word, 0)\n",
    "\n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "\n",
    "        result_dict = {}\n",
    "        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            result_dict[k] = result_dict.get(k, 0.0) + float(v)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contain_alpha(x):\n",
    "    my_re = re.compile(r'[A-Za-z]',re.S)\n",
    "    res = re.findall(my_re,x)\n",
    "    if len(res):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deal_result_list(keyword_set, sample_df):\n",
    "    temp_df = pd.DataFrame(columns=['id', 'result_list'])\n",
    "    for temp_id, jieba_title_list, result_dict, jieba_title_person_name_list, ltp_title_person_name_list, title, jieba_title_word_n_list in sample_df[['id', 'jieba_title_list', 'jieba_result_dict_10', 'jieba_title_person_name_list', 'ltp_title_person_name_list', 'title','jieba_title_word_n_list']].values:\n",
    "        show_list = list()\n",
    "        #findall(pattern, string [, flags)返回string中与pattern匹配的所有未重叠的值，包括空匹配值\n",
    "        #print(type(title))\n",
    "        word_list = re.findall(r\"《(.+?)》\", title)\n",
    "        for word in word_list:\n",
    "            if ',' in word:\n",
    "                word = word.replace(',', '，')\n",
    "                print(word)\n",
    "            show_list.append(word)\n",
    "        title_set = set(jieba_title_list)\n",
    "        keys = list(result_dict.keys())\n",
    "        title_person_name_set = set(jieba_title_person_name_list) & set(ltp_title_person_name_list)\n",
    "#         title_person_name_set = set(jieba_title_person_name_list)\n",
    "        jieba_title_word_n_set = set(jieba_title_word_n_list)\n",
    "        result_list = list()\n",
    "        result_list_1 = list()\n",
    "        result_list_2 = list()\n",
    "        result_list_3 = list()\n",
    "        result_list_4 = list()\n",
    "        result_list_5 = list()\n",
    "        result_list_6 = list()\n",
    "        result_list_7 = list()\n",
    "        for title in title_set:\n",
    "            if len(title) >= 5:\n",
    "                result_list.append(title)\n",
    "        for key in keys:\n",
    "            if key in set(result_list):\n",
    "                continue\n",
    "            if((key in keyword_set) & (key in title_set)):\n",
    "                result_list_1.append(key)\n",
    "            else:\n",
    "                if key in title_person_name_set:\n",
    "                    result_list_5.append(key)\n",
    "                else:\n",
    "                    if (((key in set(jieba_title_person_name_list)) | (key in set(ltp_title_person_name_list))) & (len(key) >= 3)):\n",
    "                        continue\n",
    "                    else:\n",
    "#                         if is_contain_alpha(key):\n",
    "#                             result_list_7.append(key)\n",
    "#                         else:\n",
    "                        if key in title_set:\n",
    "                            if key in jieba_title_word_n_set:\n",
    "                                result_list_7.append(key)\n",
    "                            else:\n",
    "                                result_list_3.append(key)\n",
    "                        else:\n",
    "                            if key in keyword_set:\n",
    "                                result_list_2.append(key)\n",
    "                            else:\n",
    "                                result_list_4.append(key)\n",
    "        for name in set(jieba_title_person_name_list):\n",
    "            if name in set(result_list_5):\n",
    "                continue\n",
    "            else:\n",
    "                if len(name) >= 3:\n",
    "                    result_list_6.append(name)\n",
    "        result_list = show_list + result_list_5 + result_list_1 + result_list + result_list_6 + result_list_7 + result_list_3 + result_list_2 + result_list_4\n",
    "        final_list = list()\n",
    "        for result in result_list:\n",
    "            if result not in final_list:\n",
    "                final_list.append(result)\n",
    "        temp = pd.DataFrame([[temp_id, final_list]], columns=['id', 'result_list'])\n",
    "        temp_df = pd.concat([temp_df, temp])\n",
    "    print(temp_df.head())\n",
    "    sample_df = pd.merge(sample_df, temp_df, on='id', how='left')\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_word(result_list, n):\n",
    "    if len(result_list) < n:\n",
    "        return '无'\n",
    "    else:\n",
    "        return result_list[n - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出预测结果\n",
    "def exportResult(df, fileName):\n",
    "    df.to_csv('../Submission/%s.csv' % fileName, header=True, index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~开始处理词典~~~~~~~~~~~~~~~~~~~\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "~~~~~~~~~~~~~~词典处理完毕~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~开始进行分词~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~分词完毕~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~开始进行tfidf统计~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~tfidf统计完毕~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~开始进行规则处理~~~~~~~~~~~~~~~~~~~\n",
      "        id                                     result_list\n",
      "0  D000001      [林志颖, 睫毛, 面膜, 太长, 老婆, 吓人, 深夜, 陈若仪, 日常, 了解]\n",
      "0  D000002    [杨幂, 身材, 太精彩, 回复, 张钧甯, 洛神, 能用, 翩若惊鸿, 眼皮, 眉宇]\n",
      "0  D000003  [年轻, 送祝福, 无人, 风华绝代, 生日, 蓝洁瑛, 美得, 妩媚动人, 55, 荧幕]\n",
      "0  D000004     [霍建华, 老公, 人气, 林心如, 私生活, 粉丝, 女儿, 索吻, 提及, 一下]\n",
      "0  D000005     [TVB, 颜值, 真老, 照曝光, 担当, 发现, 素颜, 姚莹莹, 剧集, 药房]\n",
      "~~~~~~~~~~~~~~规则处理完毕~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~完毕~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # 词典处理\n",
    "    print('~~~~~~~~~~~~~~开始处理词典~~~~~~~~~~~~~~~~~~~')\n",
    "    # 拼音表偏移，\n",
    "    startPy = 0x1540;\n",
    "\n",
    "    # 汉语词组表偏移\n",
    "    startChinese = 0x2628;\n",
    "\n",
    "    # 全局拼音表\n",
    "    GPy_Table = {}\n",
    "\n",
    "    # 解析结果\n",
    "    # 元组(词频,拼音,中文词组)的列表\n",
    "    GTable = []\n",
    "\n",
    "    # scel所在文件夹路径\n",
    "    in_path = \"../Data/dict/\"\n",
    "    # 输出词典所在文件夹路径\n",
    "    out_path = \"../Data/user_dict.txt\"\n",
    " \n",
    "    fin = [fname for fname in os.listdir(in_path) if fname[-5:] == \".scel\"]\n",
    "    for f in fin:\n",
    "        f = os.path.join(in_path, f)\n",
    "        GPy_Table, GTable = scel2txt(f, startPy, startChinese, GPy_Table, GTable)\n",
    "\n",
    "    # 保存结果\n",
    "    with open(out_path, 'w', encoding='utf8') as f:\n",
    "        f.writelines([word+'\\n' for count, py, word in GTable])\n",
    "\n",
    "    print('~~~~~~~~~~~~~~词典处理完毕~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    all_docs_df = pd.read_csv('../Data/all_docs.txt', nrows=100,sep='\\001', header=None)\n",
    "    \n",
    "    all_docs_df.columns = ['id', 'title', 'text']\n",
    "    all_docs_df['title'] = all_docs_df['title'].astype(str)\n",
    "    all_docs_df['text'] = all_docs_df['text'].astype(str)\n",
    "\n",
    "    train_doc_keyword_df = pd.read_csv('../Data/train_docs_keywords.txt', sep='\\t', header=None)\n",
    "    train_doc_keyword_df.columns = ['id', 'keyword']\n",
    "    train_doc_keyword_df['keyword_list'] = train_doc_keyword_df['keyword'].map(lambda x: x.split(','))\n",
    "    jieba.load_userdict('../Data/user_dict.txt')\n",
    "    #给jieba添加自定义词\n",
    "    for keyword_list in train_doc_keyword_df['keyword_list']:\n",
    "        for keyword in keyword_list:\n",
    "            jieba.add_word(keyword)\n",
    "    keyword_set = set()\n",
    "    for keyword_list in train_doc_keyword_df['keyword_list']:\n",
    "        for keyword in keyword_list:\n",
    "            keyword_set.add(keyword)\n",
    "    LTP_DATA_DIR = '../Model/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "    cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "    pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "    postagger = Postagger() # 初始化实例\n",
    "    postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "    print('~~~~~~~~~~~~~~开始进行分词~~~~~~~~~~~~~~~~~~~')\n",
    "    all_docs_df['text_sentences'] = all_docs_df['text'].map(lambda x: get_text_sentences(x))\n",
    "    all_docs_df['text_sentences_len'] = all_docs_df['text_sentences'].map(lambda x: len(x))\n",
    "\n",
    "    stopword_list = get_stopword_list()\n",
    "    all_docs_df['jieba_title_list'] = all_docs_df['title'].map(lambda x : jieba_word_deal(x, stopword_list, False))\n",
    "    all_docs_df['jieba_title_word_n_list'] = all_docs_df['title'].map(lambda x : jieba_title_word_n(x, stopword_list))\n",
    "    all_docs_df = get_title_text(all_docs_df)\n",
    "    all_docs_df['jieba_title_text_list'] = all_docs_df['title_text'].map(lambda x : jieba_word_deal(x, stopword_list, False))\n",
    "\n",
    "    all_docs_df['ltp_title_list'] = all_docs_df['title'].map(lambda x : ltp_word_deal(x, stopword_list, segmentor))\n",
    "    all_docs_df['ltp_title_text_list'] = all_docs_df['title_text'].map(lambda x : ltp_word_deal(x, stopword_list, segmentor))\n",
    "\n",
    "    all_docs_df['jieba_title_person_name_list'] = all_docs_df['jieba_title_list'].map(lambda x: get_title_person_name(x, postagger))\n",
    "    all_docs_df['ltp_title_person_name_list'] = all_docs_df['ltp_title_list'].map(lambda x: get_title_person_name(x, postagger))\n",
    "    print('~~~~~~~~~~~~~~分词完毕~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    print('~~~~~~~~~~~~~~开始进行tfidf统计~~~~~~~~~~~~~~~~~~~')\n",
    "    jieba_idf_dic, jieba_default_idf = train_idf(all_docs_df['jieba_title_text_list'])\n",
    "    all_docs_df['jieba_result_dict_5'] = all_docs_df['jieba_title_text_list'].map(lambda x: TfIdf(jieba_idf_dic, jieba_default_idf, x, 5).get_tfidf())\n",
    "    all_docs_df['jieba_result_dict_10'] = all_docs_df['jieba_title_text_list'].map(lambda x: TfIdf(jieba_idf_dic, jieba_default_idf, x, 10).get_tfidf())\n",
    "    ltp_idf_dic, ltp_default_idf = train_idf(all_docs_df['ltp_title_text_list'])\n",
    "    all_docs_df['ltp_result_dict_5'] = all_docs_df['ltp_title_text_list'].map(lambda x: TfIdf(ltp_idf_dic, ltp_default_idf, x, 5).get_tfidf())\n",
    "    all_docs_df['ltp_result_dict_10'] = all_docs_df['ltp_title_text_list'].map(lambda x: TfIdf(ltp_idf_dic, ltp_default_idf, x, 10).get_tfidf())\n",
    "    print('~~~~~~~~~~~~~~tfidf统计完毕~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    sample_df = pd.read_csv('../Submission/tfidf_final.csv',nrows=100, encoding='ISO-8859-1')\n",
    "    sample_df = pd.merge(sample_df, all_docs_df, on='id', how='left')\n",
    "    \n",
    "    #print(sample_df)\n",
    "    print('~~~~~~~~~~~~~~开始进行规则处理~~~~~~~~~~~~~~~~~~~')\n",
    "    sample_df = get_deal_result_list(keyword_set, sample_df)\n",
    "    sample_df['label1'] = sample_df['result_list'].map(lambda x: get_top_n_word(x, 1))\n",
    "    sample_df['label2'] = sample_df['result_list'].map(lambda x: get_top_n_word(x, 2))\n",
    "    print('~~~~~~~~~~~~~~规则处理完毕~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    exportResult(sample_df[['id', 'label1', 'label2']], 'tfidf_final1')\n",
    "    print('~~~~~~~~~~~~~~完毕~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
